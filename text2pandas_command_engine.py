# -*- coding: utf-8 -*-
"""Text2Pandas_Command_Engine

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s59Jx6whe2L0ICJ6w_NBQEz71o4cVJaa

#importing libraries
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import numpy as np
import re
import warnings
warnings.filterwarnings('ignore')
import spacy

"""# mounting Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# reading the csv files"""

df = pd.read_csv('/content/drive/MyDrive/task2.csv')
df.head()

df.info()

df.describe()

df.isnull().sum()

""" # 1 -build preprocessing function
 - this function responsible to apply some important NLP preprocessing techniques like:
    - 1-  Splitting the Query --->to handle multiple queries
    - 2-  convert it to lower case
    - 2-  Tokenization  
    - 3-  Lemmatization
    - 4-  stop words removal
    - 5-  Part-of-Speech (POS) Tagging
    - 6-  Named Entity Recognition (NER)
    - 7-  Extracting and Parsing Dates
    - 8-  Synonym Handling
    - 9-  Extracting Numerical Values
    - 9-  Negation Detection
    - 8-  Classifying Query Type
    - then Returning Structured Information
    
"""

import spacy
import re
from dateutil import parser

# Load spaCy's English model
nlp = spacy.load("en_core_web_sm")

# Define synonyms and known entities
synonyms = {
    'maximum': 'max',
    'minimum': 'min',
    'average': 'mean',
    'sum': 'sum'
}
known_entities = ['counter_0', 'counter_1', 'counter_2', 'counter_3', 'counter_4', 'counter_5']

def preprocess_query(query):
    # Split the query into multiple parts if it contains "and" or "or"
    queries = re.split(r'\band\b|\bor\b', query.lower())

    processed_queries = []

    for sub_query in queries:
        doc = nlp(sub_query)

        # Tokenization, stopword removal, and lemmatization
        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space]

        pos_tags = [(token.text, token.pos_) for token in doc if not token.is_stop]

        # Extract named entities
        named_entities = [(ent.text, ent.label_) for ent in doc.ents]

        # Extract relevant column names
        extracted_entities = [token for token in tokens if token in known_entities]

        # Extract numerical values
        numbers = [int(num) for num in re.findall(r'\d+', sub_query)]

        # Extract dates and handle relative dates (e.g., yesterday, last month)
        date_strings = [token.text for token in doc if token.ent_type_ == 'DATE']
        dates = []
        for date_string in date_strings:
          try:
            date = parser.parse(date_string, fuzzy=True)
            dates.append(date)
          except (ValueError, OverflowError):
            continue

        # Handle synonyms
        tokens = [synonyms.get(token, token) for token in tokens]

        # Detect negation
        negations = ['not', 'no', 'never', 'none', 'nothing', 'without']
        negation_present = any(token in negations for token in tokens)

        # Classify query type
        query_type = 'unknown'
        if any(op in tokens for op in ['max', 'min', 'mean', 'sum']):
            query_type = 'aggregation'
        elif 'filter' in tokens:
            query_type = 'filter'
        elif 'sort' in tokens:
            query_type = 'sorting'

        processed_queries.append({
            'tokens': tokens,
            'pos_tags': pos_tags,
            'extracted_entities': extracted_entities,
            'numbers': numbers,
            'date_times': dates,
            'query_type': query_type,
            'negation_present': negation_present
        })

    return processed_queries

# Example usage
user_query = "What is the maximum value for counter_2 for all of the cells?"
processed_queries = preprocess_query(user_query)

for i, pq in enumerate(processed_queries):
    print(f"Query {i+1}:")
    print("Tokens:", pq['tokens'])
    print("POS Tags:", pq['pos_tags'])
    print("Extracted Entities:", pq['extracted_entities'])
    print("Numbers:", pq['numbers'])
    print("Date/Times:", pq['date_times'])
    print("Query Type:", pq['query_type'])
    print("Negation Present:", pq['negation_present'])
    print()

"""#Map Natural Language Queries to Pandas Commands"""

def map_query_to_pandas(query):
    # Process the query to identify the operation and parameters
    processed_query = preprocess_query(query)[0]

    # Extract the components from the processed query
    tokens = processed_query['tokens']
    extracted_entities = processed_query['extracted_entities']
    numbers = processed_query['numbers']
    dates = processed_query['date_times']
    query_type = processed_query['query_type']
    negation_present = processed_query['negation_present']

    # Initialize the Pandas command
    pandas_command = ""

    if query_type == 'aggregation':
        # Aggregation operations like max, min, mean, sum, count
        if extracted_entities:
            column_name = extracted_entities[0]
            if 'max' in tokens:
                pandas_command = f"df.groupby('cell_no')['{column_name}'].max()"
            elif 'min' in tokens:
                pandas_command = f"df.groupby('cell_no')['{column_name}'].min()"
            elif 'mean' in tokens:
                pandas_command = f"df.groupby('cell_no')['{column_name}'].mean()"
            elif 'sum' in tokens:
                pandas_command = f"df.groupby('cell_no')['{column_name}'].sum()"
            elif 'count' in tokens:
                pandas_command = f"df.groupby('cell_no')['{column_name}'].count()"
        else:
            pandas_command = "Aggregation column not specified"

    elif query_type == 'filter':
        # Filter operations
        if extracted_entities:
            column_name = extracted_entities[0]
            value = numbers[0] if numbers else None
            if value is not None:
                if negation_present:
                    pandas_command = f"df[df['{column_name}'] != {value}]"
                else:
                    pandas_command = f"df[df['{column_name}'] == {value}]"
            else:
                pandas_command = "Filter value not specified"
        else:
            pandas_command = "Filter column not specified"

    elif query_type == 'sorting':
        # Sorting operations
        if extracted_entities:
            sort_column = extracted_entities[0]
            ascending = not ('descending' in tokens)
            pandas_command = f"df.sort_values(by='{sort_column}', ascending={ascending})"
        else:
            pandas_command = "Sorting column not specified"

    elif query_type == 'summarization':
        # Summarization operations such as summary statistics
        if extracted_entities:
            column_name = extracted_entities[0]
            pandas_command = f"df['{column_name}'].describe()"
        else:
            pandas_command = "Summarization column not specified"

    else:
        pandas_command = "Query type not recognized"

    return pandas_command

"""#Evaluating the usability and performance of the Text2Pandas command engine:"""

user_query_1 = "What is the maximum value for counter_2 for all of the cells?"
user_query_2 = "Sort the data by counter_0 in descending order"


# Map the queries to Pandas commands
print("Pandas Command for Query 1:", map_query_to_pandas(user_query_1))
print("Pandas Command for Query 2:", map_query_to_pandas(user_query_2))

"""- Discuss how you would evaluate the accuracy and efficiency of the engine, ensuring that it consistently produces correct results and handles large datasets efficiently
   - Create Diverse Test Cases

   - Manual Verification: For each test case, manually write the corresponding pandas command and compute the expected result. Compare the output from the engine with these manually calculated results.

   - Error Handling and Robustness Identify Errors: Determine if errors are due to misunderstanding queries, incorrect parsing, or mapping issues.


"""

